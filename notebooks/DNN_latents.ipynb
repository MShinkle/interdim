{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d74931a3",
   "metadata": {},
   "source": [
    "# Exploring DNN Activations with interdim\n",
    "\n",
    "In this notebook, we demonstrate how the `interdim` package can be used to visualize and explore various types of data, such as deep neural network (DNN) activations. Specifically, we will explore the internal responses of AlexNet to the FashionMNIST dataset. We will extract the responses of the 'features' module to a random subset of 1000 sample images from the dataset. Although we are using a subset for speed, the package can handle many more samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8a8f56",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Data Preparation\n",
    "\n",
    "First, we define the necessary transformations and load the FashionMNIST dataset. We will convert the grayscale images to RGB, resize them to 224x224 pixels (as expected by MobileNetV2), and normalize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a073bc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import alexnet\n",
    "import numpy as np\n",
    "from interdim import InterDimAnalysis\n",
    "from interdim.vis import InteractionPlot\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "194001e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])\n",
    "\n",
    "# Load FashionMNIST dataset with conversion to RGB\n",
    "full_trainset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data', \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=3),  # Convert grayscale to RGB\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Create a random subset of 1000 samples\n",
    "indices = np.random.choice(len(full_trainset), 1000, replace=False)\n",
    "trainset = Subset(full_trainset, indices)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb180f2c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Model Setup\n",
    "\n",
    "Next, we load a pre-trained AlexNet model and set it to evaluation mode. We also define a hook to capture the output of the 'features' module, which is the last convolutional layer of AlexNet. This layer is responsible for extracting high-level features from the input images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8939ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthew/anaconda3/envs/interdim/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/matthew/anaconda3/envs/interdim/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7764775b8a40>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load pre-trained AlexNet\n",
    "model = alexnet(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Define a hook to capture the output of a specific layer\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Register the hook\n",
    "model.features[-1].register_forward_hook(get_activation('features'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8385462",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Extracting Activations\n",
    "\n",
    "We will now pass the images through the model and extract the activations from the 'features' module. This will allow us to visualize and analyze the internal responses of the model to the FashionMNIST images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39d86b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:04<00:00,  3.53it/s]\n"
     ]
    }
   ],
   "source": [
    "latents, labels, images = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_images, batch_targets in tqdm(trainloader):\n",
    "        output = model(transform(batch_images))\n",
    "        latents.extend(activation['features'].mean(dim=[2, 3]).cpu().numpy())\n",
    "        labels.extend(batch_targets.numpy())\n",
    "        images.extend(batch_images.cpu().numpy())\n",
    "\n",
    "latents = np.array(latents)\n",
    "labels = np.array(labels)\n",
    "images = np.array(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bd0e96",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Visualizing with interdim\n",
    "\n",
    "Finally, we will use the `interdim` package to visualize the extracted activations. By comparing true labels and clustering-derived labels, we can observe how well the model's internal representations align with the actual classes in the dataset. Additionally, we can explore the structure of the activation space by hovering over the scatter plot. Check it out!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a50855",
   "metadata": {},
   "source": [
    "We'll use UMAP for this demo, which requires the `umap-learn` library. You can install it via pip if you don't have it already via the following command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533444a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install umap-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c14bc0",
   "metadata": {},
   "source": [
    "\n",
    "If you don't want to do this, you can just change the `method` argument in the `reduce` to 'tsne'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3bbd0ef",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing dimensionality reduction via UMAP with default arguments.\n",
      "Reduced data shape: (1000, 2)\n",
      "Performing clustering via DBSCAN with default arguments.\n",
      "Clustering complete. Number of clusters: 4\n",
      "Clustering Evaluation Result (adjusted_rand):\n",
      "Score: 0.23713197635088457\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:50139/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7764d7f39fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<dash.dash.Dash at 0x7764d7f3a1b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ida = InterDimAnalysis(latents, true_labels=labels, verbose=True)\n",
    "ida.reduce(method='umap', n_components=2)\n",
    "ida.cluster(method='dbscan')\n",
    "ida.score(method='adjusted_rand')\n",
    "\n",
    "interaction_plot = InteractionPlot(images*255, plot_type='image')\n",
    "ida.show(n_components=2, point_visualization=interaction_plot, marker_kwargs={\"colorscale\": 'Rainbow'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a586f19d",
   "metadata": {},
   "source": [
    "After applying the `interdim` package to these activations, we can make some interesting observations. For example, by comparing between true labels and clustering-derived labels (selected via the radio selectors above the scatter plot), we can see that we actually do a decent job of pulling out some of the clothing categories as their own clusters. This is despite 1) having provided no information to the clustering about classes in the data, and 2) the fact that AlexNet used here wasn't trained on this dataset at all! Additionally, by hovering your cursor around the scatter plot space, you can also observe rough 'axes' within these clusters. This includes axes from 'pants' to 'shirts', with dresses in between, and other axes of clothing type and light versus dark. Cool!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f9af8c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Bonus\n",
    "\n",
    "What if we did this, but with an untrained network? You can do this by setting `pretrained=False` in cell 3, and then rerunning it and the following cells. Would you expect samples to be randomly distributed throughout the activation space?\n",
    "\n",
    "Once you've done this, look at the results--there's structure here! The images aren't as clearly clustered as before, but there's still a clear class-based structure here. Furthermore, you may notice some similar axes like before by hovering your mouse around the space.\n",
    "\n",
    "### Why is this?\n",
    "<details>\n",
    "<summary>Explanation</summary>\n",
    "One possibility: Even an untrained network can exhibit some structure in its activations due to the inherent biases in the network architecture and the nature of the input data. The initial random weights can still capture some low-level features, and the network's layers can impose a form of organization on the data. Additionally, good weight initialization settings, which are designed to help the model learn more effectively, can manifest as starting the model in a 'good' spot. This initial structure likely aids in the formation of class-based clusters and axes in the activation space, even without any training.\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "interdim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
