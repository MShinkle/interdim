{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2b8e132",
   "metadata": {},
   "source": [
    "# Exploring LLM Token Embeddings and Layer Activations with interdim\n",
    "\n",
    "In this notebook, we'll explore both the token embeddings and layer activations of a language model using the `interdim` package. We'll use the `transformerlens` package to load a pre-trained model, extract its token embeddings, and then examine activations from a specific layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fa440a",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "**Note:** This notebook requires the `transformerlens` package as an additional dependency. You can install it by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8250a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformer-lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0a0c3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import transformer_lens\n",
    "from interdim import InterDimAnalysis\n",
    "from interdim.vis import InteractionPlot\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4c3efd",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8be50f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-14m into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = transformer_lens.HookedTransformer.from_pretrained(\"pythia-14m\", device=device)\n",
    "\n",
    "# Get list of all tokenizer vocabulary\n",
    "vocab = model.tokenizer.get_vocab()\n",
    "\n",
    "# Get token IDs from vocab\n",
    "token_ids = torch.tensor(list(vocab.values()), dtype=torch.long).unsqueeze(1).to(device)\n",
    "\n",
    "# Create a list of token texts\n",
    "token_texts = [text.replace('Ġ', ' ') for text in vocab.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a2ca05",
   "metadata": {},
   "source": [
    "## Part 1: Analyzing Token Embeddings\n",
    "\n",
    "First, we'll examine the token embeddings directly from the model's embedding layer. These embeddings represent the initial representation of each token before it's processed by the model's layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3298227f",
   "metadata": {},
   "source": [
    "NOTE: We'll use UMAP for this demo, which requires the `umap-learn` library. You can install it via pip if you don't have it already via the following command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "513f3b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: umap-learn in /home/matthew/anaconda3/envs/interdim/lib/python3.12/site-packages (0.5.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/matthew/anaconda3/envs/interdim/lib/python3.12/site-packages (from umap-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.3.1 in /home/matthew/anaconda3/envs/interdim/lib/python3.12/site-packages (from umap-learn) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /home/matthew/anaconda3/envs/interdim/lib/python3.12/site-packages (from umap-learn) (1.5.2)\n",
      "Requirement already satisfied: numba>=0.51.2 in /home/matthew/anaconda3/envs/interdim/lib/python3.12/site-packages (from umap-learn) (0.60.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /home/matthew/anaconda3/envs/interdim/lib/python3.12/site-packages (from umap-learn) (0.5.13)\n",
      "Requirement already satisfied: tqdm in /home/matthew/anaconda3/envs/interdim/lib/python3.12/site-packages (from umap-learn) (4.66.5)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /home/matthew/anaconda3/envs/interdim/lib/python3.12/site-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/matthew/anaconda3/envs/interdim/lib/python3.12/site-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/matthew/anaconda3/envs/interdim/lib/python3.12/site-packages (from scikit-learn>=0.22->umap-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install umap-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079d3ef6",
   "metadata": {},
   "source": [
    "\n",
    "If you don't want to do this, you can just change the `method` argument in the `reduce` to 'tsne'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ce17134",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted embeddings for 50277 tokens with shape torch.Size([50277, 128])\n",
      "Performing dimensionality reduction via UMAP with default arguments.\n",
      "Reduced data shape: (50277, 3)\n",
      "Performing clustering via BIRCH with default arguments.\n",
      "Clustering complete. Number of clusters: 281\n",
      "Visualizing Token Embeddings:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:56167/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7302c52835f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<dash.dash.Dash at 0x7302c5285a00>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract token embeddings\n",
    "with torch.no_grad():\n",
    "    token_embeddings = model.embed(token_ids).squeeze(1)\n",
    "\n",
    "print(f\"Extracted embeddings for {len(vocab)} tokens with shape {token_embeddings.shape}\")\n",
    "\n",
    "# Create the InteractionPlot for text visualization\n",
    "text_plot = InteractionPlot(\n",
    "    data_source=token_texts,\n",
    "    plot_type=\"text\",\n",
    ")\n",
    "\n",
    "# Analyze token embeddings with interdim\n",
    "ida_embeddings = InterDimAnalysis(token_embeddings.cpu().numpy(), verbose=True)\n",
    "ida_embeddings.reduce(method='umap', n_components=3)\n",
    "ida_embeddings.cluster(method='birch')\n",
    "\n",
    "# Create and display the interactive plot for token embeddings\n",
    "print(\"Visualizing Token Embeddings:\")\n",
    "ida_embeddings.show(\n",
    "    n_components=3, \n",
    "    point_visualization=text_plot,\n",
    "    marker_kwargs = {\"size\": 3, \"opacity\": 0.5, \"colorscale\": 'Rainbow'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffacd312",
   "metadata": {},
   "source": [
    "### Interpreting Token Embeddings\n",
    "\n",
    "In the plot above, each point represents a token in the model's vocabulary. The spatial arrangement reflects the relationships between tokens in the embedding space, and the embeddings show how the model initially represents tokens before any contextual processing. Do you seen any clear structure?\n",
    "\n",
    "Maybe, but it's fairly weak, with individual points having some degree of similarity with nearby points. No, how do these embeddings compare to representations *within* an LLM, after they've been processed by some of the layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868197d6",
   "metadata": {},
   "source": [
    "## Part 2: Analyzing Layer Activations\n",
    "\n",
    "Now, we'll examine the activations from a specific layer of the model. This will show us how the representations of tokens change after being processed by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1123b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 197/197 [00:29<00:00,  6.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted activations from layer blocks.5.hook_resid_post with shape torch.Size([50277, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to get activations for all tokens from a specific layer\n",
    "def get_layer_activations(model, layer_name):\n",
    "    token_ids = torch.tensor(list(vocab.values()), dtype=torch.long).unsqueeze(1).to(device)\n",
    "    \n",
    "    activations = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(torch.split(token_ids, 256)):\n",
    "            _, cache = model.run_with_cache(batch)\n",
    "            batch_activations = cache[layer_name].cpu().mean(1)  # Mean over sequence length\n",
    "            activations.append(batch_activations)\n",
    "    \n",
    "    return torch.cat(activations, dim=0)\n",
    "\n",
    "# Get activations from the last layer\n",
    "layer_name = 'blocks.5.hook_resid_post'  # Adjust this for different layers\n",
    "layer_activations = get_layer_activations(model, layer_name)\n",
    "\n",
    "print(f\"Extracted activations from layer {layer_name} with shape {layer_activations.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10d5dbcd",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing dimensionality reduction via UMAP with default arguments.\n",
      "Reduced data shape: (50277, 3)\n",
      "Performing clustering via BIRCH with default arguments.\n",
      "Clustering complete. Number of clusters: 144\n",
      "Visualizing Layer Activations from blocks.5.hook_resid_post:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:36363/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7302c4ced700>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<dash.dash.Dash at 0x7302ca14cc20>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze layer activations with interdim\n",
    "ida_activations = InterDimAnalysis(layer_activations.numpy(), verbose=True)\n",
    "ida_activations.reduce(method='umap', n_components=3)\n",
    "ida_activations.cluster(method='birch')\n",
    "\n",
    "# Create and display the interactive plot for layer activations\n",
    "print(f\"Visualizing Layer Activations from {layer_name}:\")\n",
    "ida_activations.show(\n",
    "    n_components=3, \n",
    "    point_visualization=text_plot,\n",
    "    marker_kwargs = {\"size\": 3, \"opacity\": 0.5, \"colorscale\": 'Rainbow'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea29d25",
   "metadata": {},
   "source": [
    "### Interpreting Layer Activations\n",
    "\n",
    "This plot shows the representations of tokens after they've been processed by the model up to the specified layer. Compared to the initial embeddings, you might notice:\n",
    "\n",
    "1. Different clustering patterns\n",
    "2. More nuanced relationships between tokens\n",
    "3. Potentially clearer separation between different types of tokens\n",
    "\n",
    "By comparing the token embeddings and layer activations, we can gain insights into how the model's understanding of tokens evolves through its layers as the layers use and transform these representations.\n",
    "\n",
    "Feel free to mess around by changing layers, models, etc, and seeing how these representational spaces change!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "interdim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
